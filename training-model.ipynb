{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img , img_to_array\n",
    "from tensorflow.keras.layers import Dense , Dropout , Conv2D , MaxPooling2D, Flatten , BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 23s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# load the Cifar-10 data set\n",
    "(x_train , y_train) , (x_test , y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape (50000, 32, 32, 3)\n",
      "y_train shape (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "# check the data set\n",
    "print('x_train shape' , x_train.shape)\n",
    "print('y_train shape' , y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Normalization and Augmentation\n",
    "def normalize(x):\n",
    "    x = x.astype('float32')\n",
    "    x = x/255.0\n",
    "    return x\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "                            rotation_range=15,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            horizontal_flip=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize(x_train)\n",
    "x_test = normalize(x_test)\n",
    "x_val = normalize(x_val)\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train , 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test , 10)\n",
    "y_val  = tf.keras.utils.to_categorical(y_val , 10)\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Accuracy and loss graph and model fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(model):\n",
    "  epoch = 100\n",
    "  r = model.fit(datagen.flow(x_train , y_train , batch_size = 32), epochs = epoch  ,steps_per_epoch=len(x_train)/32, validation_data = (x_val , y_val) , verbose = 1)\n",
    "  acc = model.evaluate(x_test , y_test)\n",
    "  print(\"test set loss : \" , acc[0])\n",
    "  print(\"test set accuracy :\", acc[1]*100)\n",
    "\n",
    "  epoch_range = range(1, epoch+1)\n",
    "  plt.plot(epoch_range, r.history['accuracy'])\n",
    "  plt.plot(epoch_range, r.history['val_accuracy'])\n",
    "  plt.title('Classification Accuracy')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='lower right')\n",
    "  plt.show()\n",
    "\n",
    "  # Plot training & validation loss values\n",
    "  plt.plot(epoch_range,r.history['loss'])\n",
    "  plt.plot(epoch_range, r.history['val_loss'])\n",
    "  plt.title('Model loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='lower right')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cohenniv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1562/1562 [==============================] - 47s 18ms/step - loss: 1.7082 - accuracy: 0.3915 - val_loss: 1.4599 - val_accuracy: 0.4873\n",
      "Epoch 2/100\n",
      "1562/1562 [==============================] - 21s 13ms/step - loss: 1.3948 - accuracy: 0.5058 - val_loss: 1.4511 - val_accuracy: 0.5100\n",
      "Epoch 3/100\n",
      "  52/1562 [..............................] - ETA: 19s - loss: 1.2484 - accuracy: 0.5601"
     ]
    }
   ],
   "source": [
    "# define a CNN model:\n",
    "weight_decay = 1e-4\n",
    "model = Sequential([\n",
    "                    Conv2D(32, (3, 3), activation='relu', padding='same',kernel_regularizer=tf.keras.regularizers.l2(weight_decay), input_shape=(32, 32, 3)),\n",
    "                    BatchNormalization(),\n",
    "                    Conv2D(32, (3, 3), activation='relu',kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='same'),\n",
    "                    BatchNormalization(),\n",
    "                    MaxPooling2D((2, 2)),\n",
    "                    Dropout(0.2),\n",
    "                    Conv2D(64, (3, 3), activation='relu',kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='same'),\n",
    "                    BatchNormalization(),\n",
    "                    Conv2D(64, (3, 3), activation='relu',kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='same'),\n",
    "                    BatchNormalization(),\n",
    "                    MaxPooling2D((2, 2)),\n",
    "                    Dropout(0.3),\n",
    "                    Conv2D(128, (3, 3), activation='relu',kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='same'),\n",
    "                    BatchNormalization(),\n",
    "                    Conv2D(128, (3, 3), activation='relu',kernel_regularizer=tf.keras.regularizers.l2(weight_decay), padding='same'),\n",
    "                    BatchNormalization(),\n",
    "                    MaxPooling2D((2, 2)),\n",
    "                    Dropout(0.3),\n",
    "                    Flatten(),\n",
    "                    Dense(128, activation='relu'),\n",
    "                    Dense(10, activation='softmax')                    \n",
    "])\n",
    "\n",
    "opt =    tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "results(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "model.save(\"model.hdf5\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86e6b2add717718f5bf21852a76e7c9d30624633e42f1fcdc3d858e24c77d439"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
